## Practice Problems: Vague Code and the AI Partner Problem

### Project 1: Specification Detective Tool
**Short Description:** Build a command-line tool that analyzes code prompts or requirements and identifies potential specification gaps that could lead to vague implementation.

**Step-by-Step Implementation Guidance:**
1. Create a Python script that takes a user's code requirement as input (15 min)
2. Build a checklist of common specification gaps (concurrency, error handling, edge cases) (25 min)
3. Implement functions to scan the requirement for missing elements (30 min)
4. Generate a report highlighting potential issues and suggesting clarifications (25 min)
5. Add a feature to suggest questions the user should ask before implementation (20 min)
6. Test with various example requirements and refine the detection logic (25 min)

**Real-World Use Case:** Software teams use similar tools to evaluate user stories and requirements before development sprints to prevent specification-related failures.

**Expected Learning Outcomes:**
- Understanding common specification gaps
- Learning to identify vague requirements
- Practicing proactive questioning techniques

### Project 2: AI Prompt Evaluator
**Short Description:** Create a web application that helps users improve their AI coding prompts by evaluating them for completeness and clarity.

**Step-by-Step Implementation Guidance:**
1. Set up a basic web interface with a text input for AI prompts (20 min)
2. Implement backend logic to analyze prompt completeness using predefined criteria (30 min)
3. Create a scoring system that rates prompt quality (25 min)
4. Build suggestion engine that recommends improvements (35 min)
5. Add example prompts with good vs. bad specifications for comparison (25 min)
6. Deploy the application and test with various prompts (25 min)

**Real-World Use Case:** AI development teams use similar tools to standardize prompt quality and reduce the iteration cycles needed for AI-generated code.

**Expected Learning Outcomes:**
- Understanding how to write comprehensive prompts
- Learning to anticipate edge cases
- Developing evaluation frameworks for requirements

### Project 3: Concurrent User Simulator
**Short Description:** Develop a testing tool that simulates multiple users interacting with an application to expose issues caused by vague specifications.

**Step-by-Step Implementation Guidance:**
1. Create a simple web app with a basic feature (e.g., counter, booking system) (30 min)
2. Build a simulation engine that can generate multiple simultaneous user requests (35 min)
3. Implement logging to capture race conditions and other concurrency issues (30 min)
4. Add a dashboard to visualize problems found during simulations (30 min)
5. Create before/after examples showing how better specifications prevent these issues (25 min)
6. Document findings and mitigation strategies (20 min)

**Real-World Use Case:** QA teams use similar tools to stress-test applications before they go live, preventing the kind of failures described in the lesson.

**Expected Learning Outcomes:**
- Understanding concurrency issues
- Learning to anticipate multi-user scenarios
- Appreciating the importance of detailed specifications

## Summary

These three projects will help reinforce the key concepts from the "Vague Code and the AI Partner Problem" lesson:
1. **Specification Detective Tool** - Helps identify gaps in requirements
2. **AI Prompt Evaluator** - Teaches how to write comprehensive prompts
3. **Concurrent User Simulator** - Demonstrates real-world consequences of vague specs

Each project addresses different aspects of the specification-driven development approach and provides hands-on experience with the problems that arise from "vibe coding".